{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "In this lab we will implement and train an agent that uses deep learning to play balance the stick in `CartPole-v1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "----\n",
    "We import useful packages: `gym`, `torch` stuff, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque  # for memory\n",
    "from tqdm import tqdm          # for progress bar\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How the game looks (without our agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv = gym.make('CartPole-v1', render_mode='human')\\nfor _ in tqdm(range(10)):\\n    state, _ = env.reset()\\n    done = False\\n    while not done:\\n        action = env.action_space.sample()\\n        next_state, reward, done, _, _ = env.step(action)\\nenv.close()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "for _ in tqdm(range(10)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "env.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DQN Algorithm\n",
    "-------------\n",
    "We train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is *return*. The discount, $\\gamma$ is the discount, between $0$ and $1$\n",
    "\n",
    "\n",
    "Q-learning tries to find a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, maximizes rewards:\n",
    "\n",
    "$\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}$\n",
    "\n",
    "However, we don't know $Q^*$. So, we use neural network as a approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}$\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:\n",
    "\n",
    "$\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model\n",
    "---\n",
    "Make a deep learning based policy model, that takes in a state and outputs an action.\n",
    "This model will be an attribute of the Agent we make next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        super(Model, self).__init__()\n",
    "        # initialise layers here\n",
    "        self.dense1 = nn.Linear(observation_size, 100)\n",
    "        torch.nn.init.xavier_uniform_(self.dense1.weight)\n",
    "        self.dense2 = nn.Linear(100, 100)\n",
    "        torch.nn.init.xavier_uniform_(self.dense2.weight)\n",
    "        self.dense3 = nn.Linear(100, 100)\n",
    "        torch.nn.init.xavier_uniform_(self.dense3.weight)\n",
    "        self.dense4 = nn.Linear(100, action_size)\n",
    "        torch.nn.init.xavier_uniform_(self.dense4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # send x through the network\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense4(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        #x = torch.Tensor(x)\n",
    "        x = self.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DQN Agent\n",
    "----\n",
    "We will be using experience replay memory for training our model.\n",
    "An Agent's memory is as important as its model, and will be another attribute of our agent.\n",
    "Other appropriate attributes are the hyperparameters (gamma, lr, etc.).\n",
    "Give the agent a replay method that trains on a batch from its memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logging import critical\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model = Model(observation_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.discount_value = 0.9\n",
    "        self.epsilon = 0.15\n",
    "        self.epsilondecay = 0.995\n",
    "        #When appending to deque, if we exceed the maximum size we remove the oldest element of the list\n",
    "        self.memory = deque([], maxlen=50000) # memory that stores N most new transitions\n",
    "        # good place to store hyperparameters as attributes\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # add to memory\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "\n",
    "    def act(self, state):\n",
    "        # return an action from the model\n",
    "        #print (self.epsilon)\n",
    "        if random.uniform(0,1) < self.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            action = int(action)\n",
    "            return action\n",
    "        else:\n",
    "            action = torch.argmax(self.model.predict(torch.tensor(state)))\n",
    "            action = int(action)\n",
    "            return action\n",
    "\n",
    "        #pass\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # update model based on replay memory\n",
    "        # you might want to make a self.train() helper method\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in range(batch_size):\n",
    "            self.train(minibatch[i])\n",
    "            \n",
    "        self.epsilon *= self.epsilondecay\n",
    "        self.optimizer.step()        \n",
    "\n",
    "    def train(self, sample):\n",
    "        s, a, r, st1, d = sample\n",
    "        r = torch.tensor(r)\n",
    "        if not d:\n",
    "            value = r + self.discount_value * float(torch.max(self.model.forward(torch.tensor(st1))))\n",
    "        else:\n",
    "            value = r\n",
    "        pred = self.model.forward(torch.tensor(s))[a]\n",
    "        loss = self.criterion(pred, value)\n",
    "        loss.backward()\n",
    "        return int(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main Training loop\n",
    "---\n",
    "Make a function that takes and environment and an agent, and runs through $n$ episodes.\n",
    "Remember to call that agent's replay function to learn from its past (once it has a past).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(env, agent, episodes=1000, batch_size=64):  # train for many games\n",
    "    best_reward = 0\n",
    "    for _ in tqdm(range(episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        actionLeft = 0\n",
    "        actionRight = 1\n",
    "        while not done:\n",
    "            # 1. make a move in game.\n",
    "            action = agent.act(state)\n",
    "            # 2. have the agent remember stuff.\n",
    "            #action = env.action_space.sample()\n",
    "            \n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "            # 3. update state\n",
    "            state = new_state\n",
    "            # 4. if we have enough experiences in out memory, learn from a batch with replay.\n",
    "            if len(agent.memory) >= batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        #print (\"Total reward for episode = \" + str(total_reward) + \"     ActionsLeft = \" + str(actionLeft) + \"      actionRight = \" + str(actionRight))\n",
    "        print (\"Total reward = \" + str(total_reward))\n",
    "        if total_reward >= best_reward:\n",
    "            best_reward = total_reward\n",
    "        print (\"Best reward found = \" + str(best_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Putting it together\n",
    "---\n",
    "We train an agent on the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<08:13,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 9.0\n",
      "Best reward found = 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<05:45,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 11.0\n",
      "Best reward found = 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:00<04:59,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 11.0\n",
      "Best reward found = 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:01<04:03,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 11.0\n",
      "Best reward found = 11.0\n",
      "Total reward = 8.0\n",
      "Best reward found = 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:01<03:42,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 8.0\n",
      "Best reward found = 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:02<06:40,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 9.0\n",
      "Best reward found = 11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:04<14:37,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 13.0\n",
      "Best reward found = 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [00:05<18:30,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 11.0\n",
      "Best reward found = 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [00:07<19:50,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 9.0\n",
      "Best reward found = 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:09<24:54,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 15.0\n",
      "Best reward found = 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:10<24:01,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 9.0\n",
      "Best reward found = 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [00:12<24:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 10.0\n",
      "Best reward found = 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 14/1000 [00:14<25:23,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 12.0\n",
      "Best reward found = 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/1000 [00:16<31:18,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 19.0\n",
      "Best reward found = 19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [00:18<29:50,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 11.0\n",
      "Best reward found = 19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/1000 [00:21<37:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 23.0\n",
      "Best reward found = 23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/1000 [00:27<53:56,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 39.0\n",
      "Best reward found = 39.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [00:36<1:21:46,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 70.0\n",
      "Best reward found = 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1000 [00:40<1:18:47,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 104.0\n",
      "Best reward found = 104.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [00:44<1:13:41,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 93.0\n",
      "Best reward found = 104.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1000 [00:49<1:17:03,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 128.0\n",
      "Best reward found = 128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1000 [00:56<1:24:30,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 156.0\n",
      "Best reward found = 156.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [01:03<1:36:51,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 186.0\n",
      "Best reward found = 186.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [01:09<46:50,  2.88s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumenter\\third_semester\\DLFGAS\\Exercises\\MyRepo\\dlgs\\labs\\lab5.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m, render_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(env, agent)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m torch\u001b[39m.\u001b[39msave(agent\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumenter\\third_semester\\DLFGAS\\Exercises\\MyRepo\\dlgs\\labs\\lab5.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, episodes, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# 4. if we have enough experiences in out memory, learn from a batch with replay.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#print (\"Total reward for episode = \" + str(total_reward) + \"     ActionsLeft = \" + str(actionLeft) + \"      actionRight = \" + str(actionRight))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mTotal reward = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(total_reward))\n",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumenter\\third_semester\\DLFGAS\\Exercises\\MyRepo\\dlgs\\labs\\lab5.ipynb Cell 17\u001b[0m in \u001b[0;36mAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(minibatch[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilondecay\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumenter\\third_semester\\DLFGAS\\Exercises\\MyRepo\\dlgs\\labs\\lab5.ipynb Cell 17\u001b[0m in \u001b[0;36mAgent.train\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(torch\u001b[39m.\u001b[39mtensor(s))[a]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(pred, value)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(loss)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "train(env, agent)\n",
    "env.close()\n",
    "torch.save(agent.model.state_dict(), 'model.pth')\n",
    "\n",
    "agent.model.state_dict_(torch.load('model.pth'))\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode ='human')\n",
    "for _ in tqdm(range(10)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # 1. make a move in game.\n",
    "            action = agent.act(state)\n",
    "            # 2. have the agent remember stuff\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            # 3. update state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jonas\\OneDrive\\Dokumenter\\third_semester\\DLFGAS\\Exercises\\MyRepo\\dlgs\\labs\\lab5.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(state)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m# 2. have the agent remember stuff\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39m# 3. update state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonas/OneDrive/Dokumenter/third_semester/DLFGAS/Exercises/MyRepo/dlgs/labs/lab5.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    184\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:290\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    289\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 290\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    291\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    293\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "agent.model.load_state_dict(torch.load('model.pth'))\n",
    "env = gym.make('CartPole-v1', render_mode ='human')\n",
    "for _ in tqdm(range(10)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # 1. make a move in game.\n",
    "            action = agent.act(state)\n",
    "            # 2. have the agent remember stuff\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            # 3. update state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional (highly recommended): Atari\n",
    "Adapt your agent to play an Atari game of your choice.\n",
    "https://www.gymlibrary.dev/environments/atari/air_raid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "683e9bbf599fde3b00e37a0db68ad40a268db525b46af3924c3427b16ddb8792"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
